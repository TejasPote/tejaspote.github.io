---
title: "Dynamic Forward and Backward Sparse Training (DFBST): Accelerated Deep Learning through Completely Sparse Training Schedule"
collection: publications
permalink: False
excerpt: ''
date: 2022-12-14
venue: 'Asian Conference on Machine Learning'
paperurl: 'https://proceedings.mlr.press/v189/pote23a.html'
citation: False
---
In this work, we address the shortcomings of existing dynamic sparse training methods aimed at finding sparse subnetworks within deep neural networks during training.We introduce DFBST, a novel sparse training algorithm which sparsifies both weights and gradients of the network using trainable thresholds which are jointly optimized with the network parameters through the same loss function.



[Download paper here](https://proceedings.mlr.press/v189/pote23a/pote23a.pdf)

<!-- Recommended citation: Your Name, You. (2009). "Paper Title Number 1." <i>Journal 1</i>. 1(1). -->