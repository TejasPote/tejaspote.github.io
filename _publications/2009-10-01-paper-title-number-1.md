---
title: "Dynamic Forward and Backward Sparse Training (DFBST): Accelerated Deep Learning through Completely Sparse Training Schedule"
collection: publications
permalink: https://proceedings.mlr.press/v189/pote23a.html
excerpt: 'This paper is about the number 1. The number 2 is left for future work.'
date: 2023-4-13
venue: 'Asian Conference on Machine Learning'
paperurl: 'https://proceedings.mlr.press/v189/pote23a.html'
# citation: '
# @InProceedings{pmlr-v189-pote23a,
#   title = 	 {Dynamic Forward and Backward Sparse Training
#  (DFBST): Accelerated Deep Learning through
#  Completely Sparse Training Schedule},
#   author =       {Pote, Tejas and Ganaie, Muhammad Athar and Hassan, Atif and Khare, Swanand},
#   booktitle = 	 {Proceedings of The 14th Asian Conference on Machine
#  Learning},
#   pages = 	 {848--863},
#   year = 	 {2023},
#   editor = 	 {Khan, Emtiyaz and Gonen, Mehmet},
#   volume = 	 {189},
#   series = 	 {Proceedings of Machine Learning Research},
#   month = 	 {12--14 Dec},
#   publisher =    {PMLR},
#   pdf = 	 {https://proceedings.mlr.press/v189/pote23a/pote23a.pdf},
#   url = 	 {https://proceedings.mlr.press/v189/pote23a.html},
#   abstract = 	 {Neural network sparsification has received a lot of
#  attention in recent years. A number of dynamic
#  sparse training methods have been developed that
#  achieve significant sparsity levels during training,
#  ensuring comparable performance to their dense
#  counterparts. However, most of these methods update
#  all the model parameters using dense gradients. To
#  this end, gradient sparsification is achieved either
#  by non-dynamic (fixed) schedule or computationally
#  expensive dynamic pruning schedule. To alleviate
#  these drawbacks, we propose Dynamic Forward and
#  Backward Sparse Training (DFBST), an algorithm which
#  dynamically sparsifies both the forward and backward
#  passes using trainable masks, leading to a
#  completely sparse training schedule. In contrast to
#  existing sparse training methods, we propose
#  separate learning for forward as well as backward
#  masks. Our approach achieves state of the art
#  performance in terms of both accuracy and sparsity
#  compared to existing dynamic pruning algorithms on
#  benchmark datasets, namely MNIST, CIFAR-10 and
#  CIFAR-100.}
# }
# '
# ---
This paper is about the number 1. The number 2 is left for future work.

[Download paper here](https://proceedings.mlr.press/v189/pote23a/pote23a.pdf)

Recommended citation: Your Name, You. (2009). "Paper Title Number 1." <i>Journal 1</i>. 1(1).